# Configuration for a large CLIP-conditioned DDPM model

# Model identification
model_name: "clip_ddpm_large"

# Model parameters
model:
  img_size: 32
  img_channels: 3
  down_channels: [768, 768, 1536]  # Even larger architecture
  t_embed_dim: 16
  c_embed_dim: 512  # CLIP embedding size

# Diffusion parameters
diffusion:
  timesteps: 500  # More timesteps for potentially better quality
  beta_start: 0.0001
  beta_end: 0.02

# Training parameters
training:
  epochs: 100
  batch_size: 32  # Smaller batch due to larger model
  learning_rate: 0.00005  # Lower LR for larger model
  c_drop_prob: 0.15  # Higher dropout for stronger classifier-free guidance
  save_frequency: 5
  delete_old_checkpoints: true  # Delete old checkpoints to save disk space

  # Learning rate scheduler
  lr_scheduler:
    type: "cosine"  # CosineAnnealingLR
    eta_min: 0.0000005  # Minimum learning rate

  # EMA (Exponential Moving Average)
  ema:
    enabled: true
    decay: 0.9995  # Slightly less smoothing for faster adaptation

# Data parameters
data:
  dataset_root: "../data/tiny-imagenet-200"
  use_train: true
  use_val: false
  max_samples: 100000
  preprocessed_clip: true
  clip_csv_path: "clip.csv"
  random_seed: 42

# CLIP parameters
clip:
  model_name: "ViT-B/32"
  features: 512

# Generation parameters
generation:
  text_prompts: ["goldfish swimming", "oak tree in forest", "beautiful sunset"]
  guidance_weights: [-3, -1, 0, 1, 3]  # Wider range for testing

# Paths (will be modified by model_name)
paths:
  save_dir: "../images/"
  checkpoint_dir: "../model_checkpoint/"

# Device
device: "cuda"
